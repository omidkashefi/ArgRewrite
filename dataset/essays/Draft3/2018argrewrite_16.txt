	Self-driving cars appear to be the next wave of technology that will impact our lives, following the pattern previously exploited by other technological advances. The prospect of cars that autonomously drive is an attractive one for a number of positive reasons. One argument for self-driving cars could be that they would reduce car accidents and therefore save lives. 81 percent of car crashes are the result of human error; therefore, it is appealing to think that computer systems specifically designed to combat the issues associated with human error could be good alternatives. Additionally, cars that implement some of these autonomous driving habits, such as braking and self-parking, are already on the roads. But are we truly ready for self-driving cars to be the typical mode of transportation? There are a few issues we must address before we can be sure that self-driving cars will always react the way they are programmed to and how we will respond when this differs from how a human driver would act.
	 The issue I will focus on relates to legal and moral issues that may arise due to self-driving cars. To exemplify one of these possible situations which might arise, I will present an ethical conundrum. Imagine you are sitting behind the wheel of your car, driving down the street when seemingly out of nowhere, a ball rolls out followed by a child. As humans, we must immediately react and choose to either attempt to brake and hope that you stop in time to not injure the child, or swerve the car to the side of the road and risk hitting a tree, thus injuring yourself. Because we would be required to react nearly instantly, it is difficult to blame someone legally or ethically for choosing either of these two options. However, because self-driving cars are programmed in advance, it is believed that the car would respond in the best possible option. But what is the best option here? If the car is unable to stop in time to prevent injuring the child or if the car swerves and injures the "driver" after hitting a tree, who is at fault? Is it the programmer who wrote the algorithm which resulted in the accident? Or possibly the manufacturer who built the car? There does not seem to be an easy answer to these questions, and to further complicate such scenarios, who would be held financially responsible? Car loan comparison website Auto.Loan states "Car insurance may eventually become extinct, or at least not billed to the consumer, since eventually the computer will be making all the decisions. Perhaps the premium will be paid by the car manufacturer instead of the driver". This statement seems to suggest that the car manufacturers will be responsible for paying the costs of damages, but how many self-driving car companies would be ready to take this risk and be held financially responsible for any accident caused by their cars? More importantly, should we feel safe riding in self-driving cars if they do not have the full confidence of their manufacturers or programmers to ensure their safety?
	As a possible response to quell fears over such dilemmas, one could argue that self-driving cars could be equipped with video monitoring devices to help mandate adequate driving performance. Rather than being forced to rely on humans' memory of an accident (as in the scenario above), society would be able to study the video footage to see the causes and responses that resulted in the accident, thus ensuring that the car responded appropriately. In a similar manner, this footage could then be used to better program the software in order to make self-driving cars safer for pedestrians, and also the humans sitting behind the wheel of the car.
	With the current uncertainties in mind, it is important that we consider the legal and ethical dilemmas resulting from advances in self-driving car capabilities, before we are ready to implement them throughout our society.
Self-driving cars appear to be the next wave of technology that will impact our lives, following the pattern previously exploited by the internet, cell phones, and other technological advances. The prospect of cars that autonomously drive is an attractive one for a number of positive reasons. One argument for self-driving cars could be that they would reduce car accidents and therefore save lives. 81 percent of car crashes are the result of human error; therefore, it is appealing to think that computer systems specifically designed to combat the issues associated with human error could be good alternatives. By developing specific software algorithms to respond to a myriad of situations and scenarios, self-driving cars appear to have the capability to handle any situation a human driver might encounter. Because self-driving cars would be fully automated to respond to all situations, it is comforting to think that all actions made in response to situations would lead to the best possible outcomes.

Unfortunately, we still have a way to go before these prospects and visions about self-driving cars can truly be attained. We need to spend more time considering the legal and moral dilemmas presented by self-driving cars and their actions, and how these differ from the actions taken by typical human drivers. To exemplify a possible situation in which a legal and moral dilemma might arise, I will present an ethical conundrum. Imagine you are sitting behind the wheel of your car, driving down the street. Seemingly out of nowhere, a ball rolls into the street, followed by a child chasing after it. You must make a decision as to how you will react. Your two choices consist of either attempting to brake and hope that you stop in time to not injure the child, or swerving the car to the side of the road and risk hitting a tree, thus injuring yourself. As humans in this type of situation, we would be required to react nearly instantly, therefore it is difficult to fault someone for choosing either of these two options. However, because self-driving cars are programmed in advance, we must, as a society, decide what the best action would be for the car to take. Would the programmer who decides which action the car should take be held responsible if the car is unable to stop in time to prevent injuring the child? Or would the programmer be held responsible if the car swerves and injures the "driver" after hitting a tree? There does not seem to be an easy answer to these questions, however, if we are going to instruct self-driving cars to act in ways that produce the best outcomes, it is necessary for us to decide what this looks like and how it is manifested in various real-world examples.

As a response to quell these fears, one could argue that self-driving cars could be equipped with video monitoring devices to help ensure adequate driving performance. Rather than being forced to rely on humansâ€™ memory of an accident (as in the scenario above), society would be able to study the video footage to see the causes and responses that resulted in the accident. In a similar manner, this footage could then be used to better program the software in order to make self-driving cars safer for pedestrians, and also the humans sitting behind the wheel of the car.

With these uncertainties in mind, it is important that we consider the legal and ethical dilemmas resulting from advances in self-driving car capabilities, before we are ready to implement them throughout our society.
